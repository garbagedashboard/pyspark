{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eth0      Link encap:Ethernet  HWaddr 00:0d:3a:94:51:73  \n",
      "          inet addr:10.0.0.18  Bcast:10.0.15.255  Mask:255.255.240.0\n",
      "          inet6 addr: fe80::20d:3aff:fe94:5173/64 Scope:Link\n",
      "          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n",
      "          RX packets:37331739 errors:0 dropped:1226 overruns:0 frame:0\n",
      "          TX packets:24550303 errors:0 dropped:0 overruns:0 carrier:0\n",
      "          collisions:0 txqueuelen:1000 \n",
      "          RX bytes:24660725468 (24.6 GB)  TX bytes:15173808346 (15.1 GB)\n",
      "\n",
      "lo        Link encap:Local Loopback  \n",
      "          inet addr:127.0.0.1  Mask:255.0.0.0\n",
      "          inet6 addr: ::1/128 Scope:Host\n",
      "          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n",
      "          RX packets:26665321 errors:0 dropped:0 overruns:0 frame:0\n",
      "          TX packets:26665321 errors:0 dropped:0 overruns:0 carrier:0\n",
      "          collisions:0 txqueuelen:1 \n",
      "          RX bytes:14476053440 (14.4 GB)  TX bytes:14476053440 (14.4 GB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ifconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cell magic `%%configure` not found.\n"
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.submit.pyFiles\":\"wasb://h2ofiles@****/pySparkling.egg\",\n",
    "        \"spark.master\":\"local[*]\"\n",
    "    },\n",
    "    \"driverMemory\":\"8G\",\n",
    "    \"executorMemory\":\"8G\",\n",
    "    \"numExecutors\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7faadb8270d0>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysparkling\n",
    "import os\n",
    "os.environ[\"PYTHON_EGG_CACHE\"] = \"~/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import os\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.feature import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Python version: 2.7.12 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:42:40) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Spark version: 2.0.2.2.5.4.2-7\n",
      "****************"
     ]
    }
   ],
   "source": [
    "# start Spark session\n",
    "spark = pyspark.sql.SparkSession.builder.appName('Iris').getOrCreate()\n",
    "\n",
    "# print runtime versions\n",
    "print ('****************')\n",
    "print ('Python version: {}'.format(sys.version))\n",
    "print ('Spark version: {}'.format(spark.version))\n",
    "print ('****************')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris.csv into Spark dataframe\n",
    "data = sqlContext.read.csv('wasbs://h2ofiles@****/iris.csv', header=None,inferSchema=True, nanValue=\"\")\n",
    "                         \n",
    "']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n",
    "names=['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "oldnames = data.columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+\n",
      "|_c0|_c1|_c2|_c3|        _c4|\n",
      "+---+---+---+---+-----------+\n",
      "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
      "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
      "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
      "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
      "|5.4|3.9|1.7|0.4|Iris-setosa|\n",
      "|4.6|3.4|1.4|0.3|Iris-setosa|\n",
      "|5.0|3.4|1.5|0.2|Iris-setosa|\n",
      "|4.4|2.9|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.1|1.5|0.1|Iris-setosa|\n",
      "+---+---+---+---+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "newdf = data.select([col(oldnames[index]).alias(names[index]) for index in xrange(len(oldnames))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+\n",
      "|summary|      sepal-length|        sepal-width|      petal-length|       petal-width|\n",
      "+-------+------------------+-------------------+------------------+------------------+\n",
      "|  count|               150|                150|               150|               150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|\n",
      "+-------+------------------+-------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "newdf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 rows of Iris dataset:\n",
      "+-------+--------------+\n",
      "|summary|         class|\n",
      "+-------+--------------+\n",
      "|  count|           150|\n",
      "|   mean|          null|\n",
      "| stddev|          null|\n",
      "|    min|   Iris-setosa|\n",
      "|    max|Iris-virginica|\n",
      "+-------+--------------+"
     ]
    }
   ],
   "source": [
    "print \"First 20 rows of Iris dataset:\"\n",
    "newdf.describe('class').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          class|\n",
      "+---------------+\n",
      "| Iris-virginica|\n",
      "|    Iris-setosa|\n",
      "|Iris-versicolor|\n",
      "+---------------+"
     ]
    }
   ],
   "source": [
    "newdf.select('class').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(newdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----------------+\n",
      "|sepal-length|sepal-width|petal-length|petal-width|      class|         features|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
      "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|[5.4,3.9,1.7,0.4]|\n",
      "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|[4.6,3.4,1.4,0.3]|\n",
      "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|[5.0,3.4,1.5,0.2]|\n",
      "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|[4.4,2.9,1.4,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|[4.9,3.1,1.5,0.1]|\n",
      "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|[5.4,3.7,1.5,0.2]|\n",
      "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|[4.8,3.4,1.6,0.2]|\n",
      "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|[4.8,3.0,1.4,0.1]|\n",
      "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|[4.3,3.0,1.1,0.1]|\n",
      "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|[5.8,4.0,1.2,0.2]|\n",
      "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|[5.7,4.4,1.5,0.4]|\n",
      "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|[5.4,3.9,1.3,0.4]|\n",
      "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|[5.1,3.5,1.4,0.3]|\n",
      "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|[5.7,3.8,1.7,0.3]|\n",
      "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|[5.1,3.8,1.5,0.3]|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# vectorize all numerical columns into a single feature column\n",
    "feature_cols = newdf.columns[:-1]\n",
    "assembler = pyspark.ml.feature.VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "assembler_data = assembler.transform(newdf)\n",
    "\n",
    "assembler_data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert text labels into indices\n",
    "label_data = assembler_data.select(['features'] + [ 'class'])\n",
    "label_indexer = pyspark.ml.feature.StringIndexer(inputCol='class', outputCol='label').fit(assembler_data)\n",
    "labelled_data = label_indexer.transform(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----+\n",
      "|         features|      class|label|\n",
      "+-----------------+-----------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|Iris-setosa|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|Iris-setosa|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|Iris-setosa|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|Iris-setosa|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|Iris-setosa|  0.0|\n",
      "|[5.4,3.9,1.7,0.4]|Iris-setosa|  0.0|\n",
      "|[4.6,3.4,1.4,0.3]|Iris-setosa|  0.0|\n",
      "|[5.0,3.4,1.5,0.2]|Iris-setosa|  0.0|\n",
      "|[4.4,2.9,1.4,0.2]|Iris-setosa|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|Iris-setosa|  0.0|\n",
      "|[5.4,3.7,1.5,0.2]|Iris-setosa|  0.0|\n",
      "|[4.8,3.4,1.6,0.2]|Iris-setosa|  0.0|\n",
      "|[4.8,3.0,1.4,0.1]|Iris-setosa|  0.0|\n",
      "|[4.3,3.0,1.1,0.1]|Iris-setosa|  0.0|\n",
      "|[5.8,4.0,1.2,0.2]|Iris-setosa|  0.0|\n",
      "|[5.7,4.4,1.5,0.4]|Iris-setosa|  0.0|\n",
      "|[5.4,3.9,1.3,0.4]|Iris-setosa|  0.0|\n",
      "|[5.1,3.5,1.4,0.3]|Iris-setosa|  0.0|\n",
      "|[5.7,3.8,1.7,0.3]|Iris-setosa|  0.0|\n",
      "|[5.1,3.8,1.5,0.3]|Iris-setosa|  0.0|\n",
      "+-----------------+-----------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "labelled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading for machine learning\n",
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "|[5.4,3.9,1.7,0.4]|  0.0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|\n",
      "|[5.0,3.4,1.5,0.2]|  0.0|\n",
      "|[4.4,2.9,1.4,0.2]|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|\n",
      "+-----------------+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# only select the features and label column\n",
    "processed_data = labelled_data.select(['features', 'label'])\n",
    "print(\"Reading for machine learning\")\n",
    "processed_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexer_43929aef1d4e117361db"
     ]
    }
   ],
   "source": [
    "label_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change regularization rate and you will likely get a different accuracy.\n",
    "reg = 0.01\n",
    "# load regularization rate from argument if present\n",
    "if len(sys.argv) > 1:\n",
    "    reg = float(sys.argv[1])\n",
    "\n",
    "# log regularization rate\n",
    "#run_logger.log(\"Regularization Rate\", reg)\n",
    "\n",
    "# use Logistic Regression to train on the training set\n",
    "train, test = processed_data.randomSplit([0.70, 0.30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[4.3,3.0,1.1,0.1]|  0.0|\n",
      "|[4.4,3.0,1.3,0.2]|  0.0|\n",
      "|[4.4,3.2,1.3,0.2]|  0.0|\n",
      "|[4.5,2.3,1.3,0.3]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|[4.6,3.2,1.4,0.2]|  0.0|\n",
      "|[4.6,3.6,1.0,0.2]|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|[4.7,3.2,1.6,0.2]|  0.0|\n",
      "|[4.8,3.0,1.4,0.1]|  0.0|\n",
      "|[4.8,3.4,1.6,0.2]|  0.0|\n",
      "|[4.8,3.4,1.9,0.2]|  0.0|\n",
      "|[4.9,2.4,3.3,1.0]|  1.0|\n",
      "|[4.9,2.5,4.5,1.7]|  2.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|\n",
      "|[5.0,2.0,3.5,1.0]|  1.0|\n",
      "|[5.0,2.3,3.3,1.0]|  1.0|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import linalg as mllib_linalg\n",
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_old(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return mllib_linalg.SparseVector(v.size, v.indices, v.values)\n",
    "    if isinstance(v, ml_linalg.DenseVector):\n",
    "        return mllib_linalg.DenseVector(v.values)\n",
    "    raise ValueError(\"Unsupported type {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "train_labelpoint = (train.select(col(\"label\").alias(\"label\"), col(\"features\"))\n",
    "  .rdd\n",
    "  .map(lambda row: LabeledPoint(row.label, as_old(row.features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[58] at RDD at PythonRDD.scala:48"
     ]
    }
   ],
   "source": [
    "train_labelpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LogisticRegressionWithLBFGS.train(train_labelpoint, iterations=10, numClasses=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "lrm.predict([4.3,3.0,1.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [lrm.predict(x) for x in test.select('features').rdd.map(lambda r: as_old(r[0])).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2]"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'DoubleType' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'DoubleType' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simple(x):\n",
    "    return x\n",
    "simpleUdf = udf(simple, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "lrm.predict([4.3,3.0,1.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "def predict(x):\n",
    "    a = as_old(x)\n",
    "    return lrm.predict(a)\n",
    "predictUdf = udf(predict, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+----------+\n",
      "|         features|label|prediction|\n",
      "+-----------------+-----+----------+\n",
      "|[4.4,2.9,1.4,0.2]|  0.0|         0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|         0|\n",
      "|[4.8,3.0,1.4,0.3]|  0.0|         0|\n",
      "|[4.8,3.1,1.6,0.2]|  0.0|         0|\n",
      "|[5.0,3.4,1.6,0.4]|  0.0|         0|\n",
      "|[5.1,2.5,3.0,1.1]|  1.0|         1|\n",
      "|[5.2,3.4,1.4,0.2]|  0.0|         0|\n",
      "|[5.2,4.1,1.5,0.1]|  0.0|         0|\n",
      "|[5.3,3.7,1.5,0.2]|  0.0|         0|\n",
      "|[5.4,3.0,4.5,1.5]|  1.0|         1|\n",
      "|[5.4,3.7,1.5,0.2]|  0.0|         0|\n",
      "|[5.4,3.9,1.7,0.4]|  0.0|         0|\n",
      "|[5.5,2.5,4.0,1.3]|  1.0|         1|\n",
      "|[5.5,4.2,1.4,0.2]|  0.0|         0|\n",
      "|[5.6,2.5,3.9,1.1]|  1.0|         1|\n",
      "|[5.6,2.8,4.9,2.0]|  2.0|         2|\n",
      "|[5.6,2.9,3.6,1.3]|  1.0|         1|\n",
      "|[5.6,3.0,4.1,1.3]|  1.0|         1|\n",
      "|[5.7,2.5,5.0,2.0]|  2.0|         2|\n",
      "|[5.7,2.6,3.5,1.0]|  1.0|         1|\n",
      "+-----------------+-----+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "test_predicted = test.withColumn('prediction', predictUdf(test.features)) \n",
    "test_predicted.count()\n",
    "test_predicted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import  LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "isinstance(test.features[0], MLLibVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "isinstance(test.features[0], MLVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "<type 'tuple'>"
     ]
    }
   ],
   "source": [
    "def predictionAndLabels(a,b):\n",
    "    return (a,b)\n",
    "result = predictionAndLabels(1,2)\n",
    "predictionAndLabelsUdf = udf(predictionAndLabels, IntegerType())\n",
    "print result\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "testperf =  test_predicted.withColumn('predictionAndLabels', predictionAndLabelsUdf(test_predicted.prediction, test_predicted.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabelsList = test_predicted.select('prediction','label').rdd.map(lambda r: (r[0]*1.0, r[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (0.0, 0.0), (1.0, 1.0), (2.0, 2.0), (1.0, 1.0), (1.0, 1.0), (2.0, 2.0), (1.0, 1.0), (1.0, 1.0), (0.0, 0.0), (1.0, 1.0), (2.0, 2.0), (1.0, 1.0), (2.0, 2.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (2.0, 2.0), (2.0, 2.0), (2.0, 2.0), (2.0, 2.0), (1.0, 1.0), (2.0, 2.0), (2.0, 2.0), (2.0, 2.0), (1.0, 1.0), (1.0, 1.0), (2.0, 2.0), (2.0, 2.0), (2.0, 2.0)]"
     ]
    }
   ],
   "source": [
    "predictionAndLabelsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sc.parallelize(list(predictionAndLabelsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = MulticlassMetrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 12.,   0.,   0.],\n",
      "       [  0.,  17.,   0.],\n",
      "       [  0.,   0.,  14.]])"
     ]
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falsePositiveRate is 0.0\n",
      "recall is 1.000000\n",
      "precision is 1.000000 \n",
      "fMeasure is 1.000000\n",
      "accuracy is 1.000000"
     ]
    }
   ],
   "source": [
    "print \"falsePositiveRate is \" + str(metrics.falsePositiveRate(0.0))\n",
    "print \"recall is %f\" %( metrics.recall(2.0))\n",
    "print \"precision is %f \" %(metrics.precision(1.0))\n",
    "print \"fMeasure is %f\" %(metrics.fMeasure(0.0,2.0))\n",
    "print \"accuracy is %f\" %(metrics.accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
